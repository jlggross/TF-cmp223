{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9801c03-6ccf-4bdd-9b77-5d26f4163613",
   "metadata": {},
   "source": [
    "# Trabalho Final - Parte 1 - Apresentação\n",
    "\n",
    "## Cargas de trabalho CPU-bound\n",
    "\n",
    "- Criptografia\n",
    "- Multiplicação de Matrizes\n",
    "- Ordenação (sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90a649-d1c3-44ef-ae2d-ae8485d9a34b",
   "metadata": {},
   "source": [
    "# Criptografia\n",
    "\n",
    "- AES-CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c377d1-3d3f-4fee-b804-e74fead69f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl.metadata (3.5 kB)\n",
      "Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 28.3 MB/s  0:00:00\n",
      "Installing collected packages: pycryptodome\n",
      "Successfully installed pycryptodome-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pycryptodome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1217d1-6250-4f67-a679-f5d117751f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmark: 512.0 MB ===\n",
      "Time: 2.25 s\n",
      "Throughput: 227.92 MB/s using 8 threads\n",
      "\n",
      "=== Benchmark: 1024.0 MB ===\n",
      "Time: 4.12 s\n",
      "Throughput: 248.32 MB/s using 8 threads\n",
      "\n",
      "=== Benchmark: 2048.0 MB ===\n",
      "Time: 7.63 s\n",
      "Throughput: 268.58 MB/s using 8 threads\n"
     ]
    }
   ],
   "source": [
    "# Parallel AES-CTR Benchmark Script (PyCryptodome)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from Crypto.Cipher import AES\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Parameters\n",
    "KEY = os.urandom(32)   # AES-256 key\n",
    "NONCE = os.urandom(8)  # 64-bit nonce\n",
    "THREADS = 8\n",
    "BLOCK_SIZE = 16  # AES block size in bytes\n",
    "\n",
    "def encrypt_chunk(chunk, counter_start):\n",
    "    \"\"\"Encrypt one chunk with AES-CTR using a thread-specific counter offset.\"\"\"\n",
    "    cipher = AES.new(KEY, AES.MODE_CTR, nonce=NONCE, initial_value=counter_start)\n",
    "    return cipher.encrypt(chunk)\n",
    "\n",
    "def parallel_encrypt(data, num_threads=THREADS):\n",
    "    \"\"\"Encrypt data in parallel with exactly num_threads threads.\"\"\"\n",
    "    chunk_size = len(data) // num_threads\n",
    "    chunks = [data[i*chunk_size:(i+1)*chunk_size] for i in range(num_threads)]\n",
    "    \n",
    "    # Assign counters so keystreams don't overlap\n",
    "    counters = [i * (chunk_size // BLOCK_SIZE) for i in range(num_threads)]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        encrypted_chunks = list(executor.map(encrypt_chunk, chunks, counters))\n",
    "    \n",
    "    return b''.join(encrypted_chunks)\n",
    "\n",
    "def run_benchmark(data_size_bytes):\n",
    "    \"\"\"Run encryption benchmark for a given data size (bytes).\"\"\"\n",
    "    print(f\"\\n=== Benchmark: {data_size_bytes / (1024*1024)} MB ===\")\n",
    "    data = os.urandom(data_size_bytes)  # Allocate test data\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    ciphertext = parallel_encrypt(data)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    throughput_mb_s = (data_size_bytes / (1024*1024)) / elapsed\n",
    "    \n",
    "    print(f\"Time: {elapsed:.2f} s\")\n",
    "    print(f\"Throughput: {throughput_mb_s:.2f} MB/s using {THREADS} threads\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sizes = [\n",
    "        512 * 1024 * 1024,   # 512 MB\n",
    "        1024 * 1024 * 1024,  # 1 GB\n",
    "        2 * 1024 * 1024 * 1024  # 2 GB\n",
    "    ]\n",
    "    for size in sizes:\n",
    "        run_benchmark(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "256fa69b-d4d0-41f0-8687-f710545eaa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# Correctness Check\n",
    "\n",
    "from Crypto.Cipher import AES\n",
    "\n",
    "def decrypt_chunk(chunk, counter_start):\n",
    "    cipher = AES.new(KEY, AES.MODE_CTR, nonce=NONCE, initial_value=counter_start)\n",
    "    return cipher.decrypt(chunk)\n",
    "\n",
    "def parallel_decrypt(ciphertext, num_threads=THREADS):\n",
    "    chunk_size = len(ciphertext) // num_threads\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i in range(num_threads):\n",
    "        end = start + chunk_size\n",
    "        if i == num_threads - 1:\n",
    "            end = len(ciphertext)\n",
    "        chunks.append(ciphertext[start:end])\n",
    "        start = end\n",
    "\n",
    "    blocks_per_chunk = (len(ciphertext) + num_threads - 1) // num_threads // BLOCK_SIZE\n",
    "    counters = [i * blocks_per_chunk for i in range(num_threads)]\n",
    "\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        decrypted_chunks = list(executor.map(decrypt_chunk, chunks, counters))\n",
    "\n",
    "    return b''.join(decrypted_chunks)\n",
    "\n",
    "data_size_bytes = 512 * 1024 * 1024 # 512 MB\n",
    "data = os.urandom(data_size_bytes)\n",
    "ciphertext = parallel_encrypt(data)\n",
    "recovered = parallel_decrypt(ciphertext)\n",
    "assert recovered == data, \"Decryption failed! Data does not match.\"\n",
    "print(\"Encryption verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c182356-a1a4-4565-9ef7-ccd9f59baeaa",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "-> Dask is an open-source Python library for parallel and distributed computing. It is designed to scale existing Python libraries like NumPy, Pandas, and scikit-learn to handle datasets that are larger than memory or to accelerate computations by utilizing multiple cores or machines\n",
    "\n",
    "## Install Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81dc8b1-9797-4bed-85fd-e731423e62a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2025.7.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (25.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (21.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (4.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click>=8.1->dask[complete]) (0.4.6)\n",
      "Requirement already satisfied: locket in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (2.3.2)\n",
      "Requirement already satisfied: pandas>=2.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (2.3.2)\n",
      "Requirement already satisfied: distributed==2025.7.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (2025.7.0)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (3.8.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dask[complete]) (3.1.6)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (1.1.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (7.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (3.1.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (6.5.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (2.5.0)\n",
      "Requirement already satisfied: zict>=3.0.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from distributed==2025.7.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.2 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (1.3.3)\n",
      "Requirement already satisfied: narwhals>=1.13 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (2.2.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (11.3.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (2025.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install only core parts of dask\n",
    "!pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154e88f7-a223-4c51-83e6-835b8c45b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: statistics in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.3.5)\n",
      "Requirement already satisfied: docutils>=0.3 in c:\\users\\joaol\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from statistics) (0.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil\n",
    "!pip install statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ba9a6b-a45e-4795-ab70-eea3f8f9e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created:\n",
      "   id   value\n",
      "0   0  612678\n",
      "1   1  820845\n",
      "2   2  942427\n",
      "3   3  762804\n",
      "4   4  836892\n",
      "DataFrame saved to 'data.parquet'\n",
      "Number of rows in the saved file: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd  # for reading large files efficiently\n",
    "\n",
    "# =========================\n",
    "# Configurable parameters\n",
    "N = 1_000_000 # Number of rows\n",
    "OUTPUT_FILE = \"data.parquet\"\n",
    "# =========================\n",
    "\n",
    "# 1. Create a Pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"id\": np.arange(N),\n",
    "    \"value\": np.random.randint(0, 1_000_000, size=N)\n",
    "})\n",
    "\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Save to Parquet\n",
    "df.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(f\"DataFrame saved to '{OUTPUT_FILE}'\")\n",
    "\n",
    "# 3. Read the file back with Dask and print row count\n",
    "ddf = dd.read_parquet(OUTPUT_FILE)\n",
    "print(f\"Number of rows in the saved file: {len(ddf):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f5ca5-7842-4194-a047-da878b602b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Sort\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# =========================\n",
    "# Configurable parameters\n",
    "NUM_WORKERS = 8        # Number of Dask workers (processes)\n",
    "NUM_PARTITIONS = 8     # Number of partitions for the Dask DataFrame\n",
    "PARQUET_FILE = \"data.parquet\"\n",
    "SORT_COLUMN = \"value\"\n",
    "OUTPUT_FILE = \"sorted_data.parquet\"\n",
    "N_ITERATIONS = 10     # Number of sorting attempts\n",
    "# =========================\n",
    "\n",
    "# Start Dask client\n",
    "with Client(n_workers=NUM_WORKERS, threads_per_worker=1) as client:\n",
    "\n",
    "    # Load Parquet\n",
    "    ddf = dd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "    # Repartition to the desired number of partitions\n",
    "    ddf = ddf.repartition(npartitions=NUM_PARTITIONS)\n",
    "\n",
    "    # Store times for each iteration\n",
    "    elapsed_times = []\n",
    "\n",
    "    for i in range(1, N_ITERATIONS + 1):\n",
    "        #print(f\"\\n--- Iteration {i} ---\")\n",
    "\n",
    "        # Make a copy of the Dask DataFrame before sorting\n",
    "        ddf_copy = ddf.copy()\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Sort the copy\n",
    "        sorted_ddf = ddf_copy.sort_values(by=SORT_COLUMN)\n",
    "\n",
    "        # Trigger computation\n",
    "        result = sorted_ddf.compute()\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        elapsed_times.append(elapsed)\n",
    "\n",
    "        print(f\"Iteration {i} completed in {elapsed:.5f} seconds\")\n",
    "\n",
    "    # Compute statistical metrics\n",
    "    mean_time = statistics.mean(elapsed_times)\n",
    "    median_time = statistics.median(elapsed_times)\n",
    "    stdev_time = statistics.stdev(elapsed_times) if len(elapsed_times) > 1 else 0\n",
    "    min_time = min(elapsed_times)\n",
    "    max_time = max(elapsed_times)\n",
    "    variance_time = statistics.variance(elapsed_times) if len(elapsed_times) > 1 else 0\n",
    "\n",
    "    print(\"\\n=== Statistical Summary ===\")\n",
    "    print(f\"Mean time       : {mean_time:.5f} seconds\")\n",
    "    print(f\"Median time     : {median_time:.5f} seconds\")\n",
    "    print(f\"Standard dev    : {stdev_time:.5f} seconds\")\n",
    "    print(f\"Variance        : {variance_time:.5f}\")\n",
    "    print(f\"Minimum time    : {min_time:.5f} seconds\")\n",
    "    print(f\"Maximum time    : {max_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515f5c5-f739-487d-b710-1aaa086237be",
   "metadata": {},
   "source": [
    "# Multiplicação de Matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2571e4b0-0405-4b10-9a50-64e1ec23a71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client started with 8 workers\n",
      "Using chunk size: 800 x 800\n",
      "Matrix multiplication completed\n",
      "Result shape: (800, 800)\n",
      "Elapsed time: 0.31 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dask Matrix Multiplication Script\n",
    "---------------------------------\n",
    "- Matrix size is configurable via MATRIX_SIZE variable (default 800)\n",
    "- Uses Dask arrays for blocked / parallel multiplication\n",
    "- Creates exactly 8 Dask workers (processes), each using 1 thread\n",
    "- Each block multiplication uses NumPy / BLAS internally\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "MATRIX_SIZE = 2000       # Size of NxN matrices\n",
    "NUM_WORKERS = 8          # Number of Dask worker processes\n",
    "THREADS_PER_WORKER = 1   # Threads per worker (BLAS / NumPy)\n",
    "BLOCK_MEMORY_GB = 1      # Approx memory per block in GB\n",
    "\n",
    "# -----------------------------\n",
    "# Set BLAS threading environment variables (to avoid oversubscription)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(THREADS_PER_WORKER)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(THREADS_PER_WORKER)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(THREADS_PER_WORKER)\n",
    "\n",
    "# -----------------------------\n",
    "# Start Dask client\n",
    "client = Client(n_workers=NUM_WORKERS, threads_per_worker=THREADS_PER_WORKER)\n",
    "print(\"Dask client started with 8 workers\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute safe chunk size based on memory per block\n",
    "chunk_elements = int(np.sqrt(BLOCK_MEMORY_GB * (1024**3) / 8))\n",
    "chunk_size = min(chunk_elements, MATRIX_SIZE)\n",
    "print(f\"Using chunk size: {chunk_size} x {chunk_size}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Create Dask arrays (random matrices)\n",
    "A = da.random.random((MATRIX_SIZE, MATRIX_SIZE), chunks=(chunk_size, chunk_size))\n",
    "B = da.random.random((MATRIX_SIZE, MATRIX_SIZE), chunks=(chunk_size, chunk_size))\n",
    "\n",
    "# -----------------------------\n",
    "# Matrix multiplication\n",
    "start_time = time.time()\n",
    "C = A @ B\n",
    "result = C.compute()  # Trigger parallel execution\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Matrix multiplication completed\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# -----------------------------\n",
    "# Clean up\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefcf64-95f5-43b3-8cd0-586fc6695c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
